\begin{thebibliography}{}

\bibitem[Abadi et~al., 2016]{abadi2016tensorflow}
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M.,
  Ghemawat, S., Irving, G., Isard, M., et~al. (2016).
\newblock Tensorflow: A system for large-scale machine learning.
\newblock In {\em 12th $\{$USENIX$\}$ Symposium on Operating Systems Design and
  Implementation ($\{$OSDI$\}$ 16)}, pages 265--283.

\bibitem[Ambikasaran et~al., 2014]{ambikasaran2014fast}
Ambikasaran, S., Foreman-Mackey, D., Greengard, L., Hogg, D.~W., and O'Neil, M.
  (2014).
\newblock Fast direct methods for gaussian processes.
\newblock {\em arXiv preprint arXiv:1403.6015}.

\bibitem[Andrieu et~al., 2003]{andrieu2003introduction}
Andrieu, C., De~Freitas, N., Doucet, A., and Jordan, M.~I. (2003).
\newblock An introduction to mcmc for machine learning.
\newblock {\em Machine learning}, 50(1-2):5--43.

\bibitem[Arbenz et~al., 2012]{arbenz2012lecture}
Arbenz, P., Kressner, D., and Z{\"u}rich, D. (2012).
\newblock Lecture notes on solving large scale eigenvalue problems.
\newblock {\em D-MATH, EHT Zurich}, 2.

\bibitem[Babak and Deutsch, 2009]{babak2009statistical}
Babak, O. and Deutsch, C.~V. (2009).
\newblock Statistical approach to inverse distance interpolation.
\newblock {\em Stochastic Environmental Research and Risk Assessment},
  23(5):543--553.

\bibitem[Bauer et~al., 2016]{bauer2016understanding}
Bauer, M., van~der Wilk, M., and Rasmussen, C.~E. (2016).
\newblock Understanding probabilistic sparse gaussian process approximations.
\newblock In {\em Advances in neural information processing systems}, pages
  1533--1541.

\bibitem[Betancourt, 2017]{betancourt2017conceptual}
Betancourt, M. (2017).
\newblock A conceptual introduction to hamiltonian monte carlo.
\newblock {\em arXiv preprint arXiv:1701.02434}.

\bibitem[Blei et~al., 2017]{blei2017variational}
Blei, D.~M., Kucukelbir, A., and McAuliffe, J.~D. (2017).
\newblock Variational inference: A review for statisticians.
\newblock {\em Journal of the American Statistical Association},
  112(518):859--877.

\bibitem[Bottou, 2010]{bottou2010large}
Bottou, L. (2010).
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In {\em Proceedings of COMPSTAT'2010}, pages 177--186. Springer.

\bibitem[Camilla, 2018]{camillagpr}
Camilla, T. (2018).
\newblock {\em Computational Aspects of Gaussian Process Regression}.
\newblock PhD thesis, University of Amsterdam.

\bibitem[Carr and Madan, 1999]{carr1999option}
Carr, P. and Madan, D. (1999).
\newblock Option valuation using the fast fourier transform.
\newblock {\em Journal of computational finance}, 2(4):61--73.

\bibitem[Chan et~al., 1994]{chan1994circulant}
Chan, R.~H., Nagy, J.~G., and Plemmons, R.~J. (1994).
\newblock Circulant preconditioned toeplitz least squares iterations.
\newblock {\em SIAM Journal on Matrix Analysis and Applications}, 15(1):80--97.

\bibitem[Chen and Wang, 2018]{chen2018priors}
Chen, Z. and Wang, B. (2018).
\newblock How priors of initial hyperparameters affect gaussian process
  regression models.
\newblock {\em Neurocomputing}, 275:1702--1710.

\bibitem[Damianou and Lawrence, 2013]{damianou2013deep}
Damianou, A. and Lawrence, N. (2013).
\newblock Deep gaussian processes.
\newblock In {\em Artificial Intelligence and Statistics}, pages 207--215.

\bibitem[De~Spiegeleer et~al., 2018]{de2018machine}
De~Spiegeleer, J., Madan, D.~B., Reyners, S., and Schoutens, W. (2018).
\newblock Machine learning for quantitative finance: fast derivative pricing,
  hedging and fitting.
\newblock {\em Quantitative Finance}, 18(10):1635--1643.

\bibitem[Dong et~al., 2017]{dong2017scalable}
Dong, K., Eriksson, D., Nickisch, H., Bindel, D., and Wilson, A.~G. (2017).
\newblock Scalable log determinants for gaussian process kernel learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6327--6337.

\bibitem[Duchi, 2007]{duchi2007derivations}
Duchi, J. (2007).
\newblock Derivations for linear algebra and optimization.
\newblock {\em Berkeley, California}, 3:2325--5870.

\bibitem[Fichtner et~al., 2018]{fichtner2018tutorial}
Fichtner, A., Zunino, A., and Gebraad, L. (2018).
\newblock A tutorial introduction to the hamiltonian monte carlo solution of
  weakly nonlinear inverse problems.

\bibitem[Gardner et~al., 2018a]{gardner2018gpytorch}
Gardner, J., Pleiss, G., Weinberger, K.~Q., Bindel, D., and Wilson, A.~G.
  (2018a).
\newblock Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu
  acceleration.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  7576--7586.

\bibitem[Gardner et~al., 2018b]{gardner2018product}
Gardner, J.~R., Pleiss, G., Wu, R., Weinberger, K.~Q., and Wilson, A.~G.
  (2018b).
\newblock Product kernel interpolation for scalable gaussian processes.
\newblock {\em arXiv preprint arXiv:1802.08903}.

\bibitem[Gormley, 2017]{pptkmeans}
Gormley, M. (2017).
\newblock Clustering (k-means).
\newblock
  https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture15-cluster.pdf.

\bibitem[Hensman et~al., 2013]{hensman2013gaussian}
Hensman, J., Fusi, N., and Lawrence, N.~D. (2013).
\newblock Gaussian processes for big data.
\newblock {\em arXiv preprint arXiv:1309.6835}.

\bibitem[Hensman et~al., 2015]{hensman2015mcmc}
Hensman, J., Matthews, A.~G., Filippone, M., and Ghahramani, Z. (2015).
\newblock Mcmc for variationally sparse gaussian processes.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1648--1656.

\bibitem[Heston, 1993]{heston1993closed}
Heston, S.~L. (1993).
\newblock A closed-form solution for options with stochastic volatility with
  applications to bond and currency options.
\newblock {\em The review of financial studies}, 6(2):327--343.

\bibitem[Hoffman et~al., 2013]{hoffman2013stochastic}
Hoffman, M.~D., Blei, D.~M., Wang, C., and Paisley, J. (2013).
\newblock Stochastic variational inference.
\newblock {\em The Journal of Machine Learning Research}, 14(1):1303--1347.

\bibitem[Hoffman and Gelman, 2014]{hoffman2014no}
Hoffman, M.~D. and Gelman, A. (2014).
\newblock The no-u-turn sampler: adaptively setting path lengths in hamiltonian
  monte carlo.
\newblock {\em Journal of Machine Learning Research}, 15(1):1593--1623.

\bibitem[Hutchinson, 1990]{hutchinson1990stochastic}
Hutchinson, M.~F. (1990).
\newblock A stochastic estimator of the trace of the influence matrix for
  laplacian smoothing splines.
\newblock {\em Communications in Statistics-Simulation and Computation},
  19(2):433--450.

\bibitem[Keys, 1981]{keys1981cubic}
Keys, R. (1981).
\newblock Cubic convolution interpolation for digital image processing.
\newblock {\em IEEE transactions on acoustics, speech, and signal processing},
  29(6):1153--1160.

\bibitem[Kingma and Ba, 2014]{kingma2014adam}
Kingma, D.~P. and Ba, J. (2014).
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}.

\bibitem[Kingma et~al., 2015]{kingma2015variational}
Kingma, D.~P., Salimans, T., and Welling, M. (2015).
\newblock Variational dropout and the local reparameterization trick.
\newblock In {\em Advances in neural information processing systems}, pages
  2575--2583.

\bibitem[Krishnamoorthy and Menon, 2013]{krishnamoorthy2013matrix}
Krishnamoorthy, A. and Menon, D. (2013).
\newblock Matrix inversion using cholesky decomposition.
\newblock In {\em 2013 Signal Processing: Algorithms, Architectures,
  Arrangements, and Applications (SPA)}, pages 70--72. IEEE.

\bibitem[Kuss, 2006]{Kuss:06}
Kuss, M. (2006).
\newblock {\em Gaussian Process Models for Robust Regression, Classification,
  and Reinforcement Learning}.
\newblock PhD thesis, Technische Universit{\"a}t Darmstadt.

\bibitem[Lalchand and Rasmussen, 2019]{lalchand2019approximate}
Lalchand, V. and Rasmussen, C.~E. (2019).
\newblock Approximate inference for fully bayesian gaussian process regression.
\newblock {\em arXiv preprint arXiv:1912.13440}.

\bibitem[Liu and Nocedal, 1989]{liu1989limited}
Liu, D.~C. and Nocedal, J. (1989).
\newblock On the limited memory bfgs method for large scale optimization.
\newblock {\em Mathematical programming}, 45(1-3):503--528.

\bibitem[Madan et~al., 1998]{madan1998variance}
Madan, D.~B., Carr, P.~P., and Chang, E.~C. (1998).
\newblock The variance gamma process and option pricing.
\newblock {\em Review of Finance}, 2(1):79--105.

\bibitem[Models-GPM, 2013]{modelscomparative}
Models-GPM, G.~P. (2013).
\newblock A comparative evaluation of stochastic-based inference methods for
  gaussian process models.

\bibitem[Murray and Adams, 2010]{murray2010slice}
Murray, I. and Adams, R.~P. (2010).
\newblock Slice sampling covariance hyperparameters of latent gaussian models.
\newblock In {\em Advances in neural information processing systems}, pages
  1732--1740.

\bibitem[Nielsen, 2015]{nielsen2015neural}
Nielsen, M.~A. (2015).
\newblock {\em Neural networks and deep learning}, volume 2018.
\newblock Determination press San Francisco, CA, USA:.

\bibitem[Palczewski, 2016]{palczewski2016numerical}
Palczewski, J. (2016).
\newblock Numerical schemes for sdes.
\newblock {\em The University of Leeds-School of Mathematics. Lecture notes
  MATH5350 (Computations in Finance)}.

\bibitem[Qui{\~n}onero-Candela and Rasmussen, 2005]{quinonero2005unifying}
Qui{\~n}onero-Candela, J. and Rasmussen, C.~E. (2005).
\newblock A unifying view of sparse approximate gaussian process regression.
\newblock {\em Journal of Machine Learning Research}, 6(Dec):1939--1959.

\bibitem[Rasmussen and Williams, 2006]{GPRbook}
Rasmussen, C.~E. and Williams, C. K.~I. (2006).
\newblock {\em Gaussian Processes for Machine Learning}.
\newblock the MIT Press: (Cambridge, MA).

\bibitem[Riley et~al., 2006]{riley2006mathematical}
Riley, K.~F., Hobson, M.~P., and Bence, S.~J. (2006).
\newblock {\em Mathematical methods for physics and engineering: a
  comprehensive guide}.
\newblock Cambridge university press.

\bibitem[Saat{\c{c}}i, 2012]{saatcci2012scalable}
Saat{\c{c}}i, Y. (2012).
\newblock {\em Scalable inference for structured Gaussian process models}.
\newblock PhD thesis, Citeseer.

\bibitem[Salimbeni and Deisenroth, 2017]{salimbeni2017doubly}
Salimbeni, H. and Deisenroth, M. (2017).
\newblock Doubly stochastic variational inference for deep gaussian processes.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4588--4599.

\bibitem[Salvatier et~al., 2016]{salvatier2016probabilistic}
Salvatier, J., Wiecki, T.~V., and Fonnesbeck, C. (2016).
\newblock Probabilistic programming in python using pymc3.
\newblock {\em PeerJ Computer Science}, 2:e55.

\bibitem[Sch{\"o}lkopf et~al., 2001]{scholkopf2001generalized}
Sch{\"o}lkopf, B., Herbrich, R., and Smola, A.~J. (2001).
\newblock A generalized representer theorem.
\newblock In {\em International conference on computational learning theory},
  pages 416--426. Springer.

\bibitem[Schoutens, 2008]{wimschoutensvg}
Schoutens, W. (2008).
\newblock The world of vg.
\newblock In {\em M. Vanmaele, D. Deelstra, A. De Schepper, J. Dhaene, en P.
  Van Goethem, editors, Handelingen Contactforum Actuarial and Financial
  Mathematics Conference, paginaâ€™s}, pages 3--54.

\bibitem[Sejdinovic and Gretton, 2012]{sejdinovic2012rkhs}
Sejdinovic, D. and Gretton, A. (2012).
\newblock What is an rkhs?

\bibitem[Shewchuk et~al., 1994]{shewchuk1994introduction}
Shewchuk, J.~R. et~al. (1994).
\newblock An introduction to the conjugate gradient method without the
  agonizing pain.

\bibitem[Snelson and Ghahramani, 2006]{snelson2006sparse}
Snelson, E. and Ghahramani, Z. (2006).
\newblock Sparse gaussian processes using pseudo-inputs.
\newblock In {\em Advances in neural information processing systems}, pages
  1257--1264.

\bibitem[Titsias, 2009]{titsias2009variational}
Titsias, M.~K. (2009).
\newblock Variational model selection for sparse gaussian process regression.
\newblock {\em Report, University of Manchester, UK}.

\bibitem[Ubaru et~al., 2017]{ubaru2017fast}
Ubaru, S., Chen, J., and Saad, Y. (2017).
\newblock Fast estimation of tr(f(a)) via stochastic lanczos quadrature.
\newblock {\em SIAM Journal on Matrix Analysis and Applications},
  38(4):1075--1099.

\bibitem[Wang et~al., 2019]{wang2019exact}
Wang, K., Pleiss, G., Gardner, J., Tyree, S., Weinberger, K.~Q., and Wilson,
  A.~G. (2019).
\newblock Exact gaussian processes on a million data points.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  14622--14632.

\bibitem[Welling, 2013]{welling2013kernel}
Welling, M. (2013).
\newblock Kernel ridge regression.
\newblock {\em Max Welling's Classnotes in Machine Learning}, pages 1--3.

\bibitem[Wilson and Nickisch, 2015]{wilson2015kernel}
Wilson, A. and Nickisch, H. (2015).
\newblock Kernel interpolation for scalable structured gaussian processes
  (kiss-gp).
\newblock In {\em International Conference on Machine Learning}, pages
  1775--1784.

\bibitem[Wilson, 2014]{wilson2014covariance}
Wilson, A.~G. (2014).
\newblock {\em Covariance kernels for fast automatic pattern discovery and
  extrapolation with Gaussian processes}.
\newblock PhD thesis, University of Cambridge.

\bibitem[Wilson et~al., 2015]{wilson2015thoughts}
Wilson, A.~G., Dann, C., and Nickisch, H. (2015).
\newblock Thoughts on massively scalable gaussian processes.
\newblock {\em arXiv preprint arXiv:1511.01870}.

\bibitem[Wilson et~al., 2016a]{wilson2016deep}
Wilson, A.~G., Hu, Z., Salakhutdinov, R., and Xing, E.~P. (2016a).
\newblock Deep kernel learning.
\newblock In {\em Artificial Intelligence and Statistics}, pages 370--378.

\bibitem[Wilson et~al., 2016b]{wilson2016stochastic}
Wilson, A.~G., Hu, Z., Salakhutdinov, R.~R., and Xing, E.~P. (2016b).
\newblock Stochastic variational deep kernel learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2586--2594.

\bibitem[Yang, 2017]{yang2017understanding}
Yang, X. (2017).
\newblock Understanding the variational lower bound.

\end{thebibliography}
